# 西瓜书代码实战

> 本教程为周志华老师的《机器学习》（简称“西瓜书”）🍉配套代码实战教程，GitHub链接：https://github.com/datawhalechina/machine-learning-toy-code
>
> 【作者简介】
>
> 牧小熊，Datawhale成员&武汉分部负责人，招联金融数据工程师
>
> 知乎主页：https://www.zhihu.com/people/muxiaoxiong
>
> 【相关推荐】
>
> 西瓜书配套公式推导教程——🎃南瓜书：https://github.com/datawhalechina/pumpkin-book

# 第1章绪论

Sklearn（Scikit-Learn）是一个基于Python语言的强大的机器学习工具库，它建立在NumPy、SciPy、Pandas和Matplotlib等科学计算库之上，提供了一整套机器学习算法和数据预处理功能。

Sklearn 因其易用性、统一而优雅的API设计、丰富的算法支持以及活跃的社区，成为许多机器学习开发者和研究人员的首选工具。具体如下：

1. **主要功能**：Sklearn包含了六大任务模块：分类、回归、聚类、降维、模型选择和预处理。这些模块涵盖了机器学习实验的主要步骤，从数据预处理到模型的选择和评估。
2. **算法支持**：Sklearn提供了丰富的机器学习算法，包括线性模型（如线性回归）、决策树、支持向量机（SVM）、随机森林、K-means聚类和主成分分析（PCA）等。这些算法在监督学习（分类和回归）、无监督学习（聚类和降维）中都有广泛应用。
3. **特点优势**：Sklearn的设计强调一致性、可检验、标准类、可组合和默认值等原则。这意味着不同功能的API有统一的调用方式，便于用户快速上手并灵活应用。
4. **数据处理**：在数据预处理方面，Sklearn具备强大的功能，包括数据清洗、标准化、特征编码、特征提取等。这使得从原始数据到可用于模型训练的格式转换变得简单高效。
5. **模型评估**：为了帮助用户评估和优化模型性能，Sklearn提供了多种工具，包括交叉验证、网格搜索以及各种性能指标（如准确率、召回率和F1分数）。
6. **社区更新**：Sklearn拥有活跃的社区支持，并且持续更新。最新的版本要求使用Python 3.6或更高版本，保持与当前技术环境的兼容性。

sklearn安装

```Plain
pip install -U scikit-learn

conda install scikit-learn
```

**关联链接：**

https://scikit-learn.org/stable/index.html

https://www.scikitlearn.com.cn/

# 第2章模型评估与选择

## 2.2.1留出法

> “留出法”直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集，另一个作为测试集

train_test_split方法能够将数据集按照用户的需要指定划分为训练集和测试集

| train_data   | 待划分的样本特征集合                                         |
| ------------ | ------------------------------------------------------------ |
| X_train      | 划分出的训练数据集数据                                       |
| X_test       | 划分出的测试数据集数据                                       |
| y_train      | 划分出的训练数据集的标签                                     |
| y_test       | 划分出的测试数据集的标签                                     |
| test_size    | 若在0~1之间，为测试集样本数目与原始样本数目之比；若为整数，则是测试集样本的数目 |
| random_state | 随机数种子，不同的随机数种子划分的结果不同                   |
| stratify     | stratify是为了保持split前类的分布，例如训练集和测试集数量的比例是 A：B= 4：1，等同于split前的比例（80：20）。通常在这种类分布不平衡的情况下会用到stratify。 |

假设我们现在有数据集D，将数据集按照0.8:0.2的比例划分为数据集与测试集

```Python
from sklearn.datasets import load_iris

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## 2.2.2 交叉验证法

> 要在sklearn中实现交叉验证，可以使用KFold函数

```Python
from sklearn.model_selection import KFold
from sklearn.datasets import load_iris

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 定义模型
# 假设现在有一个模型model
model 

# 定义K折交叉验证
kf = KFold(n_splits=5)

# 进行K折交叉验证
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # 训练模型
    model.fit(X_train, y_train)
    
    # 预测测试集
    y_pred = model.predict(X_test)
    
```

## 2.2.3自助法

```Python
import numpy as np
from sklearn.utils import resample

#假设有一个数据集X和对应的标签y
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
y = np.array([0, 1, 0, 1, 0])

#使用自助法进行抽样
bootstrap_samples = []
for _ in range(100):  # 生成10个自助样本
    X_resampled, y_resampled = resample(X, y)
    bootstrap_samples.append((X_resampled, y_resampled))
    
#bootstrap_samples现在包含了100个自助样本，每个样本都是原始数据集的一个随机子集
```

## 2.3性能度量

### 均方误差

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=OTkwM2NhYmMzYTA4MDNmNTM5MzJlODdjYTFiMGEwNjBfMVJER3FOMEVoRHlySlJ0NXNxN2Eyb0pZUnY5SW14ZFhfVG9rZW46QXdnTWJjZWhibzJackh4MW9OZWN5YXdzbnpoXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#使用sklearn包实现
from sklearn.metrics import mean_squared_error

y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

mse = mean_squared_error(y_true, y_pred)
print("MSE:", mse)
```

### 错误率

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=NzJlYWE1Mjg3ZDA1MGI4Yzk5MDBiZDdhYjUwNjgxYjdfSHBhU0s1YjFiWnc5Z2tOeDJVRVRaVE9MVEJTM2VxUjVfVG9rZW46RU9OdWJyU3JSb1UwTWx4SWh2UmNrVkVQbm1lXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
def error_rate(y_pred, y_true):
    """
    计算错误率的函数。
    
    参数：
    predicted (list): 预测值列表
    actual (list): 实际值列表
    
    返回：
    float: 错误率
    """
    total = len(y_pred)
    errors = sum(1 for p, a in zip(y_pred, y_true) if p != a)
    error_rate = errors / total
    return error_rate

# 示例
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]
error_rate = error_rate(y_pred , y_true )
print("错误率：", error_rate)

#错误率可以用1-精度来获得
```

### 精度

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=YTljNzkzNGQ5OTZkNWVlYWJhMDMwYmFmYjcyMWYyYThfelhkam5vdXJjNVlvamJQMWh3a0Y2SkV1eU5ZVmtkNVJfVG9rZW46RVVRU2JCZG9ab3Z5WjF4Mlc5Q2NoM0RyblNnXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#使用sklearn 计算精度
from sklearn.metrics import accuracy_score

# 假设y_true是实际标签，y_pred是预测标签
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]

# 计算准确率
accuracy = accuracy_score(y_true, y_pred)
print("精度：", accuracy)
```

### 查准率

$${precision=\frac{{TP}}{{TP+FP}}}$$

```Python
##########
from sklearn.metrics import precision_score

# 假设y_true是真实的标签列表，y_pred是预测的标签列表
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 0, 1, 0, 1]

# 计算精确率
precision = precision_score(y_true, y_pred)

print("精确率：", precision)
```

### 查全率

$${Recall=\frac{{TP}}{{TP+FN}}}$$

```Python
###################
from sklearn.metrics import recall_score

# 假设y_true是真实的标签列表，y_pred是预测的标签列表
y_true = [1, 0, 1, 1, 0, 1]
y_pred = [1, 1, 0, 1, 0, 1]

# 计算召回率
recall = recall_score(y_true, y_pred)

print("召回率：", recall)
```

### F1值

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=MGJmOWY3NGEyYzQwOWJjMTE5ZTJkMDNhZjBkMjY2MGFfNGV3RDlITkFTZHNHdkdwRDZlUVh0S2hOSnBWZkVZVHFfVG9rZW46UzlmVmJXb25Ub2MxQnJ4S2VYemNUVEJnbk1nXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#使用sklearn 计算F1值
from sklearn.metrics import f1_score
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 1, 0, 1, 0]
f1 = f1_score(y_true, y_pred)
print("F1值：", f1)
```

### AUC值

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=N2UyMjNjZDgyOTI1YzRhNTRkOTdiNjg1ZWM5ODhjMTBfSG5DcGpmWktKOU9kSEcwWk5ydjNVV2xINWZwZXl6SFhfVG9rZW46VVZKVGJtZkttb0VRSzZ4NTV2U2MzRjlkblRlXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
#使用sklearn 计算AUC值
from sklearn.metrics import roc_auc_score

# 假设y_true是真实标签，y_pred是预测概率
from sklearn.metrics import roc_auc_score
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

# 计算AUC值
auc = roc_auc_score(y_true, y_pred)
print("AUC值为：", auc)
```

## 2.5偏差与方差

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDZiMjUxOTFlMzBlOWU0ZWVkMGMyNDM1ZmQ0MDA5YWJfSU82MFVMa0tIUThwWm9XcmhhbVlGckxMbWRyWnRXRzRfVG9rZW46U0UyTGJzNVFYb3liWmJ4QXNYSWNXTWgzbjhnXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
import math
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

bias_2=sum([(x - y) ** 2 for x, y in zip(y_true , y_pred )])
bias=math.sqrt(bias_2)
```

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=NDYzY2MzM2UzODhlMjY2YmIzNGQ5YTFiZDU5ZjEzZWFfbVhONnREZmhqbmlGQTdNZGpiQzhIdlhkbzRGSzVPdGxfVG9rZW46UDN0NmJzS09Eb1RFeXh4eTRzNmM0d3FablhRXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

```Python
from sklearn.metrics import variance_score
y_true = [1, 0, 1, 1, 0]
y_pred = [0.9, 0.8, 0.7, 0.6, 0.5]

# 计算方差
var= variance_score(y_true, y_pred)
print("方差：", var)
```

# 第3章线性模型

本节主要实现线性模型中的线性回归与逻辑回归

## 线性回归

线性回归是比较简单的机器学习算法，通常作为机器学习入门第一课。

线性回归的一般形式为

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=NGI0ZWJmNDJhMzVjYzcxOGU5MmVkMGYwZmZiMmM0MTFfRDRBWWNQNkZSc3IyUFpRYkFzWjJGeXlsbXM4ckRJSWhfVG9rZW46TnVqSGI1VDBnbzlsZll4WUNlcGNrdlhjbmZnXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjFmNGExYjQ1NmUzMWRmOGM5Nzg4MDU2ZDlhNzllYmNfNzZQaW1yN1VBRUU2dlhKREMwVzhXRGZ3MjVoV3RNMnpfVG9rZW46R1ZYU2JDbmNNb0h3bnJ4M2pYWmNCUWwybndiXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

通常情况下系数w和b 无法之间求解，我们往往会用梯度下降法进行解决，具体而言，就是给w 和 b 一个初始值，计算均方误差的梯度，从而继续更新参数，于是对上面的3.4的公式求偏导可以得到

​                                     $${\frac{{ \partial loss}}{{ \partial w}}={\mathop{ \sum }\limits_{{i=1}}^{{m}}{2 \left( wx\mathop{{}}\nolimits_{{i}}+b-y\mathop{{}}\nolimits_{{i}} \left) x\mathop{{}}\nolimits_{{i}}\right. \right. }}}$$

​                                     $$ {\frac{{ \partial loss}}{{ \partial b}}={\mathop{ \sum }\limits_{{i=1}}^{{m}}{2 \left( wx\mathop{{}}\nolimits_{{i}}+b-y\mathop{{}}\nolimits_{{i}} \right) }}}$$                       

### 构造数据集

```Python
import numpy as np 

#生成一个样本量为100 特征量为10的数据集
sample=100
feature=10
X=np.random.rand(sample,10)
#构造目标值 y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音
#np.random.randn 生成满足正态分布的随机数
```

### 构建模型

```Python
class LR():
    def __init__(self):
        self.w=None
        self.b=None 
        
    def get_loss(self,y,y_pre):
        """
        定义损失函数f(x)--> 均方误差
        y: 真实值
        y_pre: 预测值
        """
        loss=np.mean((y-y_pre)**2)
        return loss
    
    def fit(self,x,y,learning_rate=0.01,n_iterations=500):
        """
        x 训练数据
        y 目标值
        learning_rate 学习率
        n_iterations 迭代次数
        """
        sample,feature=x.shape
        
        if self.w==None:
            #初始化数据
            self.w=np.random.randn(feature)
            self.b=0
            
        #开始训练
        for i in range(n_iterations):
            y_pre=np.dot(x,self.w)+self.b
            #计算预测值与真实值之差  
            # !!注意这里是预测值减真实值
            diff=y_pre-y
            #计算损失函数的梯度
            dw=2/sample*np.dot(x.T,diff)
            db=2/sample*np.sum(diff,axis=0)
            
            #更新参数
            self.w=self.w-learning_rate*dw
            self.b=self.b-learning_rate*db
            
            #计算loss 
            loss=self.get_loss(y,y_pre)
            print("epoch:{}  loss:{}".format(i,loss))
                  
    def predict(self,x):
        y_pre=np.dot(x,self.w)+self.b
        return y_pre
```

### 预测结果

```Python
#生成一个样本量为100 特征量为10的数据集
sample=1
feature=10
test_x=np.random.rand(sample,10)
#构造目标值 y=3+2*x1+5*x2-3*x3
test_y=3+2*test_x[:,0]+5*test_x[:,1]-3*test_x[:,2]+np.random.randn(sample) # 增加一点噪音
#np.random.randn 生成满足正态分布的随机数

model=LR()
model.fit(X,y)
y_pre=model.predict(test_x)
```

### 使用sklearn包实现

```Python
#构造数据集
import numpy as np 

#生成一个样本量为100 特征量为10的数据集
sample=100
feature=10
X=np.random.rand(sample,feature)
#构造目标值 y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音
#np.random.randn 生成满足正态分布的随机数

from sklearn.linear_model import LinearRegression # 线性回归模型
from sklearn.metrics import mean_squared_error # 评价指标

model=LinearRegression() 
model.fit(X,y)
y_pre=model.predict(test_x)

#显示权重系数
weights = model.coef_
intercept = model.intercept_

print("权重：", weights)
print("偏移：", intercept)
```

**参数讲解**

fit() 函数中带的参数

- fit_intercept: 是否计算截距。默认为 True，表示计算截距。
- normalize: 是否在回归前对数据进行归一化。默认为 False。
- copy_X: 是否复制X。默认为 True。
- n_jobs: 用于计算的作业数。默认为 None，表示使用1个作业。如果设置为 -1，则使用所有CPU。

## 逻辑回归

### Sigmoid 函数

![img](https://d9ty988ekq.feishu.cn/space/api/box/stream/download/asynccode/?code=ODFmZjE0Y2QyYzQ3MTg5OTFlMmNkMjBlMTczZGZkMjhfZ3NwQ0NSZHZ0S3o4UlczMzVKbDJpZnU0UmYzb1J1b3BfVG9rZW46R2lEQmJ4c1Vab2Ztc3J4TURJcmM5cGhJbklkXzE3MjQ3NjMyODc6MTcyNDc2Njg4N19WNA)

实现sigmod函数

```Python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 示例
x = np.array([5])
print(sigmoid(x)) 
```

### 构造数据集

```Python
import numpy as np 
#构造数据集
#生成一个样本量为100 特征量为10的数据集
sample=100
feature=10
X=np.random.rand(sample,10)
#构造目标值 y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音
# 我们需要构建一个分类的问题
mean_=np.mean(y)
# 一半的数据低于平均值 一半的数据高于平均值
y=np.where(y>=mean_,1,0)
#样本就变成了分类事实，数据是否比平均值大
"""
array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])
"""

#构造数据集
```

### 使用sklearn包实现

```Python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np 
#构造数据集
#生成一个样本量为100 特征量为10的数据集
sample=100
feature=10
X=np.random.rand(sample,10)
#构造目标值 y=3+2*x1+5*x2-3*x3
y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音
# 我们需要构建一个分类的问题
mean_=np.mean(y)
# 一半的数据低于平均值 一半的数据高于平均值
y=np.where(y>=mean_,1,0)
#样本就变成了分类事实，数据是否比平均值大

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# 创建逻辑回归模型
log_reg = LogisticRegression()

# 训练模型
log_reg.fit(X_train, y_train)

# 预测
predictions = log_reg.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)
```

**参数讲解**

fit() 函数中带的参数

1. **基本参数**

   1. 1.  **penalty**：该参数用于指定正则化项的类型，可选值为“l1”、“l2”和“elasticnet”，默认为“l2”。L1正则化倾向于生成稀疏权重矩阵，有助于特征选择；L2正则化则倾向于避免模型权重过大，从而防止过拟合。

   2. 1.  **C**：此参数是正则化强度的倒数，即惩罚系数λ的倒数。较小的C值表示正则化强度更大，会使模型更简单，提高泛化能力。

   3. 1.  **fit_intercept**：该布尔参数决定是否在模型中计算截距项，即是否需要偏置，默认为True。

2. **求解器相关参数**

   1. 1.  **solver**：该参数用于选择优化算法，不同的算法适用于不同的情况。例如，“liblinear”适合小数据集或L1正则化；“lbfgs”、“newton-cg”和“sag”适合大数据集且仅支持L2正则化；“saga”则既适用于L1也适用于L2正则化。

   2. 1.  **max_iter**：这是最大迭代次数，用于指定优化算法的收敛阈值，即达到多少次迭代后停止训练。

3. **容忍度及随机状态**

   1. 1.  **tol**：该参数用于设置求解器的容忍度，即多小的变化会被认为是收敛，不再继续迭代。

   2. 1.  **random_state**：该参数用于设置随机种子，确保每次运行的结果可复现。

4. **多元分类策略**

   1. 1.  **multi_class**：当处理多分类问题时，该参数决定了采用何种策略，"ovr"（one-vs-rest）或者"multinomial"（many-vs-many），前者在每个二元分类问题上独立训练一个分类器，后者则同时考虑所有类别。

5. **对偶及权重参数**

   1. 1.  **dual**：该参数只在使用“liblinear”求解器并选择L2正则化时有用，当样本数大于特征数时建议设置为False。

   2. 1.  **class_weight**：用于设定各类别的权重，可以是字典形式，也可以是“balanced”让类库自动计算权重，特别适用于处理类别不平衡的数据。

6. **其他参数：**

   1.  **verbose：**对于liblinear和lbfgs求解器，将verbose设置为任何正数以表示详细程度。用于开启/关闭迭代中间输出的日志。

   2.  **n_jobs**：如果multi_class =‘ovr’，则在对类进行并行化时使用的CPU数量。 无论是否指定’multi_class’，当`solver`设置为’liblinear’时，都会忽略此参数。 如果给定值-1，则使用所有CPU。